---
layout: post
title: "AGU fall meeting: Scrapping the data"
published: true
---

American Geophysical  Union (AGU)  meeting is a  geoscience conference
hold  each year  around Christmas  in San  Francisco. It  represents a
great opportunity for PhD students like  me to show off their work and
enjoy what the west coast has to offer.

![AGU logo](https://meetings.agu.org/meetings/files/2014/04/fm300x200.jpg)

However, with  nearly 24 000 attendees,  AGU Fall Meeting is  also the
largest Earth  and Space  science meeting  in the  world. As  such, it
represents an  interesting data set to  explore the important  trend in
the geoscience academic  world. This year, I decided to  step back and
look at the mix of more than 23 000 oral and poster presentations from
a data science perspective.

In  this   post,  I  explain  how   I  gathered  the  data   from  the
[Science meeting website](https://fallmeeting.agu.org/2015/scientific-program/).

## Scrapping the data from the official website

I usually  used **mechanize** as  a browser simulator to  download the
page and  **BeautifulSoup** to parse  the HTML content.   However, the
[scientific   program](https://agu.confex.com/agu/fm15/meetingapp.cgi)
results to  be mainly JavaScript  calls and this strategy  simply did
not work in this case.   This is because **mechanize** generates an
HTTP  request to  get  the  web page  and  deliver  the received  HTML
directly.    However,   the   pieces   of   information   are   mainly
auto-generated by a  **JavaScript** code embedded in the  HTML page in
our case which is not supported by **mechanize**.

Some  googling later,  I discover  that  **selenium** is  a much  more
better option  when scrapping complicated **HTML**  pages.  Instead of
simulating  a browser,  **selenium** allows  you to  directly interact
with **chromium**, **safari**  or whatever browser you  are using when
surfing the web.

Using **selenium**, the vanilla code to scrap a web page looks like

{% highlight python %}

from selenium.webdriver.support.ui import WebDriverWait
from selenium import webdriver
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

# open the browser
wd = webdriver.Chrome(os.path.join(PATH_TO_DRIVER, 'chromedriver'))

# visit the page 
wd.get(link)

# wait for it to generate a specific element
WebDriverWait(wd, timeout).until(
            EC.visibility_of_element_located((By...., ...)))

# scrap the element
data = wd.find_element_by....().text

# quit when you are done
wd.quit()
{% endhighlight %}

And that's all you need. 

Modern browsers  usually provide  a way  for the  user to  inspect the
**HTML** code to identify the  different elements contained within the
page. For instance,  in **chrome**, visit the page, right  click on it
and select **inspect** to get the full structure of the web page. This
way,  it is  easy to  identify  the elements  in the  content you  are
interested  in. 

The **webdriver** gets a very reach  API to gather the desired content
either by *tag*, *class* or *id*. In our case, we are going to use the
*class*  name.   For instance,  the  contribtuion  abstract is  always
contained  in  an  element  class  named  **Additional**.   Therefore,
gathering       the       abstract        of       this       specific
[contribution](https://agu.confex.com/agu/fm15/meetingapp.cgi/Paper/67077)
translates in python code to

{% highlight python %}
wd = webdriver.Chrome(os.path.join(PATH_TO_DRIVER, 'chromedriver'))
link = 'https://agu.confex.com/agu/fm15/meetingapp.cgi/Paper/67077'
wd.get(link)
WebDriverWait(wd, 3).until(EC.visibility_of_element_located((By.CLASS_NAME, 'Additional')))
abstract = wd.find_element_by_class_name('Additional').text
wd.quit()
{% endhighlight %}

For each contribution, I finally decide to collect all the information
available, i.e.   namely the *tag*, *title*,  *date*, *time*, *place*,
*abstract*, *reference*, *authors*, *session*  and *section*. The data
are                                                          available
[here](https://github.com/cthorey/agu_data/tree/master/agu_data/Data/)
for both  2014 and 2015 as  **json** files, each of  them containing a
chunk of 1000 contributions. The code  to donwload the data as well as
to     read     the     json      files     are     also     available
[here](https://github.com/cthorey/agu_data/tree/master/agu_data).



One thing which is missing in each contribution page is the country of
origin of each participant. 

I  also run  a  similar scrapping  procedure on  what's  look like  an
annuary of all AGU members contained  in the **Person** section of the
website.  Indeed, I wanted the  origin information on each participant
which is  not provided on the  Paper section of the  webpage.  I first
try to get  it from the home institution  address gathered previously
using **geopy**.  However, I had poor  success using this method and I
decide  it  would  be  much  easier   to  get  it  directly  from  the
website. The resulting data set is available here.


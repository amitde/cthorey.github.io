---
layout: post
title: "AGU fall meeting: Part 1"
published: true
---

American  Geophiscal Union  (AGU) meeting  is a  geocsience conference
hold  each year  around christmas  in San  Francisco. It  represents a
great oppportunity for phd students like  me to show off their work and
enjoy what the west coast has to offer.

![AGU logo](https://meetings.agu.org/meetings/files/2014/04/fm300x200.jpg)

However, with  nearly 24 000 attendees,  AGU Fall Meeting is  also the
largest Earth  and space  science meeting  in the  world. As  such, it
represents an interesting dataset to  explore what's trendy and what's
not in  the geocsience academic  world. This  year, I decided  to step
back  and  look  at the  mix  of  more  than  23,000 oral  and  poster
presentations from the data science perspective.

In  this   post,  I  explain  how   I  gathered  the  data   from  the
[Science meeting website](https://fallmeeting.agu.org/2015/scientific-program/). 

## Scrapping the data from the official website

I usually  used **mechanize** as  a browser simulator to  download the
page and  **BeautifulSoup** to parse  the html content.   However, the
scientific program results  to be mainly javascript calls  and, as you
can see below, the usual following command don't return anything good.
This is because they generate an HTTP  request to get the web page and
deliver the received HTML directly. However, the pieces of information
are mainly  auto-generated by  a **JavaScript**  code emmbeded  in the
HTML page in our case which is not supported by **mechanize**. 

Some  googling later,  I discover  that  **selenium** is  a much  more
better option  when scrapping  complicated **HTML** pages.  Instead of
simulating a  browser, **selenium**  allows to directly  interact with
**chromium**,  **safari**  or  whatever  browser you  are  using  when
surfing the web. 

Using **selenium**, the vanilla code to scrap is

    # open the browser
    wd = webdriver.Chrome(os.path.join(PATH_TO_DRIVER, 'chromedriver'))
    # visit the page 
    wd.get(link)
    # scrap what you want
    data = wd.find_element_by....().text
    # quit when you are done
    wd.quit()

And that's all you need. 

Modern browser  usually provides  a way  for the  user to  inspect the
**HTML** code to  identify the different balises within  the page. For
instance, in  **chrome**, just visit the  page, right click on  it and
select **inspect** to get the full structure of the webpage. This way,
it is easy  to identify the balise of the  content your are interested
in. The **webdriver** allows you to gather the content either by *tag*,

The **webdriver** gets a very reach  API to gather the desired content
either by *tag*, *class* or *id*. In our case, we are going to use the
*class* name.   For instance, the  abstract content is contained  in a
balise class **Additional** and gathering  the abstract content of the
10000 contribution translates in python code to

    wd = webdriver.Chrome(os.path.join(PATH_TO_DRIVER, 'chromedriver'))
    link = 'https://agu.confex.com/agu/fm15/meetingapp.cgi/Paper/10000'
    wd.get(link)
    abstract = wd.find_element_by_class_name('Additional').text
    wd.quit()

For each contribution, I finally decide to collect all the information
availables from the *tag* name at the top to the name of the *session*
at the bottom of the page. Namely, for every contribution, I collect
*tag*,  *title*,  *date*,  *time*, *place*,  *abstract*,  *reference*,
*authors*, *session* and *section*. The data is available **here** and
the code to generate the data can be download on github. 

I  also run  a  similar scrapping  procedure on  what's  look like  an
annuary of all AGU members contained  in the **Person** section of the
website.  Indeed, I wanted the  origin information on each participant
which is  not provided on the  Paper section of the  webpage.  I first
try to get  it from the home institution  address gathered precedently
using **geopy**.  However, I had poor  success using this method and I
decide  it  would  be  much  easier   to  get  it  directly  from  the
website. The resulting dataset is available here.



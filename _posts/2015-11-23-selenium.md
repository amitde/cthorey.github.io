---
layout: post
title: "selenium"
published: true
---

With nearly 24  000 attendees, AGU Fall Meeting is  one of the largest
Earth and space science meeting in  the world. This year, I decided to
step back  and look  at the mix  of more than  23,000 oral  and poster
presentations from the data science perspective. A first step, and the
subject  of this  post, will  be  gathering the  information for  each
presentation                          from                         the
[AGU website](https://fallmeeting.agu.org/2015/scientific-program/).

![enter image description here](https://meetings.agu.org/meetings/files/2014/04/fm300x200.jpg)

I usually  used **mechanize** as  a browser simulator to  download the
page and  **BeautifulSoup** to  parse the  html content.  However, the
scientific program result  to be mainly javascript  calls. Indeed, the
content is not directly coded in the HTML DOM tree but

What should  be taken into  account when  the content is  not directly
coded in the HTML DOM tree?  The main difference, as you probably have
already  noted,  is  that  using the  downloading  methods  that  were
suggested in  the previous article  (urllib2 or mechanize)  just don't
work. This  is because they  generate an HTTP  request to get  the web
page  and  deliver   the  received  HTML  directly   to  the  scraping
script. However, the pieces of  information that are auto-generated by
the JavaScript code are  not yet in the HTML file  because the code is
not executed  in any virtual  machine as it  happens when the  page is
displayed in a web browser.

<iframe
src="http://bl.ocks.org/mbostock/raw/4061502/0a200ddf998aa75dfdb1ff32e16b680a15e5cb01/"
marginwidth="0" marginheight="0" scrolling="no"></iframe>


---
layout: post
title: "AGU fall meeting: Part 3 - Abstract similarities"
published: true
---

American Geophysical  Union (AGU)  meeting is a  geoscience conference
hold  each year  around Christmas  in San  Francisco. It  represents a
great opportunity for PhD students like  me to show off their work and
enjoy what  the west coast has  to offer. However, with  nearly 24 000
attendees,  AGU Fall  Meeting  is  also the  largest  Earth and  space
science meeting  in the world.  As such, it represents  an interesting
data set to dive into the geoscience academic
world.

In  this post,  I explore  different information  retrieval techniques
taken from  the field  of natural language  processing to  explore the
hidden patterns in the submitted abstract collection in 2015.

The objective is two fold:

- Identify  semantic-based   similarities  between   the  contribution
  proposed  at AGU  to  build  a recommandation  system  based on  the
  abstract content.
- Propose for each contributor a list of potential collaborators based
  on the authors of the papers proposed by our recommendantion system.

Different natural language processing tools are available in python to
achieve        this        goal         and        after        trying
[sklearn](http://scikit-learn.org/stable/),  I  decided to  settle  on
[gensim](https://radimrehurek.com/gensim/)  which has  partilarly fast
and  effective  implementations to  work  with  large dataset  (~20000
abstracts here).

The basic stage, which I'll detail in the following are

- Cleaning the data.
- Construct a valid embedding for the corpus.
- Compute the similarities between the document within this embedding.

## Data cleaning

Data  cleaning  is  an  essential   step  for  a  good  recommendation
system.  Indeeed, our  model is  going to  use the  corpus to  build a
consistent embedding of  the abstracts and we don't want  him to focus
on   unnecessary  details.   In   particular,  I   used  the   module
**unicodedata** to remove non-ascii caracters from the corpus.

{% highlight python %}
data = get_all_data('agu2015')
sources = [df for df in data if (''.join(df.title) != "") and (df.abstract != '') and (len(df.abstract.split(' '))>100)]
abstracts = get_clean_abstracts(sources)
titles = get_clean_titles(sources)
{% endhighlight %}

In the  following, I'll use  one of  my contributions to  evaluate the
consistency of our recommendation system.

{% highlight python %}
def name_to_idx(sources,name):
    ''' From an authors, return the list of contributions '''
    contrib = [f for f in sources if name in f.authors.keys()]
    return [sources.index(elt) for elt in contrib]
    
my_contrib = name_to_idx(sources,'Clement Thorey')
print 'Title : %s'%(titles[my_contrib[0]])
print 'Abstract : %s'%(abstracts[my_contrib[0]])+'\n\n'
{% endhighlight %}

`Title  : Floor-Fractured  Craters through  Machine Learning  Methods`

`Abstract :  Floor-fractured craters are impact  craters that have
    undergone  post impact  deformations.  They  are characterized  by
    shallow floors with a plate-like  or convex appearance, wide floor
    moats,     and      radial,     concentric,      and     polygonal
    floor-fractures. While  the origin of these  deformations has long
    been  debated, it  is now  generally  accepted that  they are  the
    result  of the  emplacement of  shallow magmatic  intrusions below
    their floor.  These  craters thus constitute an  efficient tool to
    probe  the  importance  of  intrusive  magmatism  from  the  lunar
    surface. The most recent  catalog of lunar-floor fractured craters
    references  about 200  of them,  mainly located  around the  lunar
    maria Herein,  we will  discuss the  possibility of  using machine
    learning algorithms  to try to detect  new floor-fractured craters
    on the Moon among the 60000  craters referenced in the most recent
    catalogs. In particular, we will use the gravity field provided by
    the Gravity Recovery and  Interior Laboratory (GRAIL) mission, and
    the  topographic dataset  obtained  from the  Lunar Orbiter  Laser
    Altimeter  (LOLA) instrument  to  design a  set of  representative
    features for each crater. We  will then discuss the possibility to
    design a binary supervised classifier, based on these features, to
    discriminate between  the presence  or absence  of crater-centered
    intrusion  below  a  specific   crater.   First  predictions  from
    different classifier  in terms  of their accuracy  and uncertainty
    will be presented.`
    
May be  a bit of context  can be usefull  here.  My PhD was  about the
detection  and   the  characterization   of  magmatic   intrusions  on
terrestrial planets  with a special focus  on the Moon. For  those who
wonder, a magmatic intrusion is a large volume of magma which, instead
of  rising until  the surface  and form  a volcano,  emplace at  depth
beneath  the  surface  (less  than  a  few  km)  where  it  cools  and
solidifies.   On Earth,  erosion and  weathering can  sometimes expose
these intrusions at the surface. This  is the case for instance in the
henry mountains as shown in the picture. 

![Example of an exposed magmatic intrusion in the Henry Mountains](https://upload.wikimedia.org/wikipedia/commons/a/a6/Laccolith_Montana.jpg)

My contribution  at AGU  deals with  the detection  of floor-fractured
craters which  are preferential sites  for intrusive magmatism  on the
Moon. Indeed, those craters are  initial impact craters that have been
heavily deformed following the emplacement of large volume of magma (a
few tens of km$^3$) below their floor.  The resulting crater structures are
abnormally shallow and  crossed by important network  of fractures. In
this  contribution,  I  use  machine learning  techniques  to  try  to
automatically  detect potential  floor-fractured  craters among  60000
referenced  lunar impact  craters (For  more detailed,  the poster  is
available [here](https://agu.confex.com/agu/fm15/mediafile/Handout/Paper67077/Poster_CM_reduced.pdf)).

![A lunar floor-fractured crater](http://farm4.staticflickr.com/3707/9570048249_0f15000ace_b.jpg)


## Bag of Words model

The basic  representation for a  corpus of  text document is  called a
[Bag of Word (BoW) model](https://en.wikipedia.org/wiki/Bag-of-words_model). This
model  looks  at  all the  words  in  the  corpus  and first  build  a
dictionary referencing all  the words it has seen by  an index.  Then,
for each document in the corpus,  it simply counts how many times each
word of the dictionary appears in this particular document. The result
is  a  large matrice  of  count  where each  row  is  a word  from  the
dictionary and each columns is a particular document of the corpus. As
you can guess, the matrice is mostly fill with zeros.

### Tokenizer

Under the hood, the BoW  model assumes an efficient tokenizer function
which is  able to  split each  document it  its own  set of  tokens. A
vanilla tokenizer function looks like this

{% highlight python %}
def tokenizer(text):
    return text.split(' ')
{% endhighlight %}

which simply looks at each document and  splits it in a list of tokens
according to  the white spaces  in the  document. In the  following, I
will use  a slightly more  evolved version  of this tokenizer  which I
embedded in a `Tokenizer` class.

{% highlight python %}
class Tokenizer(object):

    def __init__(self, add_bigram):
        self.add_bigram = add_bigram
        self.stopwords = nltk.corpus.stopwords.words('english')
        self.stemmer = nltk.stem.snowball.SnowballStemmer("english")

    def bigram(self, tokens):
        if len(tokens) > 1:
            for i in range(0, len(tokens) - 1):
                yield tokens[i] + '_' + tokens[i + 1]

    def tokenize_and_stem(self, text):
        tokens = [word.lower() for sent in nltk.sent_tokenize(text)
                  for word in nltk.word_tokenize(sent)]
        filtered_tokens = []
        bad_tokens = []
        # filter out any tokens not containing letters (e.g., numeric tokens, raw
        # punctuation)
        for token in tokens:
            if re.search('(^[a-z]+$|^[a-z][\d]$|^[a-z]\d[a-z]$|^[a-z]{3}[a-z]*-[a-z]*$)', token):
                filtered_tokens.append(token)
            else:
                bad_tokens.append(token)
        filtered_tokens = [
            token for token in filtered_tokens if token not in self.stopwords]
        stems = map(self.stemmer.stem, filtered_tokens)
        if self.add_bigram:
            stems += [f for f in self.bigram(stems)]
        return map(str, stems)
{% endhighlight %}

The core of  this tokenizer class lies  within the `tokenize_and_stem`
function. This function  uses the [nltk](http://www.nltk.org/) library
to first  break each document  (abstract) into sentences,  then words.
Next, using simple regular expressions,  it keeps only suitable tokens
and remove all the others.

In particular,

- `^[a-z]+$` keeps only words made of letters.
- `^[a-z][\d]$` selects tokens that have 2 characters, one letter, one number (molecule stuff).
- `^[a-z][\d][a-z]$` selects tokens that have 3 characters, one letter, one number, one letter (again molecule stuff).
- `^[a-z]{3}[a-z]*-[a-z]*$` includes some tokens that are composed of two words joined by -.

Next, I  use a **stopword**  list provided  by the **nltk  module** to
filter  out all  the common  words of  the english  language.  Indeed,
while  words  like  'the'  or  'as' are  most  likely  to  be  present
everywhere, they do not carry  meaningfull information in our purpose.
Finally,  I  also incorporates  a  last  stage  of stemming  for  each
token. Stemming is the term  used in information retrieval to describe
the  process of  reducing  words  to their  word  stem,  base or  root
formâ€”generally a written word form.

For instance, imagine this document

"Here we show that running is good for health. Indeed runner are quite
healthy. Though they  have runned a lot in their  runly life, they are
quite good at that."

Clearly, this document is all about running! Nevertheless, without the
stemming  part in  our tokenizer,  'runly' will  have the  same weight
(count) than 'good', equal to 1. In contrast, the stemming will reduce
'running',  'runned',  'runly'  and  'runner' to  their  stem,  namely
'run'. The word  'run' in the BoW  model will then have a  weight of 4
for  this  document  clearly  underlying its  importance!  I  use  the
so-called  **SnowballStemmer** included  in the  library **nltk**  for
stemming.

Note that the last part of the function also allows the possibility to
incorporate  bi-grams  in   the  final  list  of   tokens,  i.e.   all
combinations of two consecutive stem-words  in the document which is a
common practise when using the BoW model.

### Dictionnary

From there,  we need to build  a dictionary of all  possible tokens in
the  corpus.   **Gensim**  is  built  in  a  memory-friendly  fashion.
Therefore, instead of loading the whole corpus into memory, tokenizing
and stemming  everything and see what  remains, it allows us  to build
the dictionary document by document, with  one document in memory at a
time.


{% highlight python %}
# Path to work in 
abstractf = os.path.join(model_saved,'gensim','abstract','abstract')

# First, write the document corpus on a txt file, one document per line.
write_clean_corpus(abstracts,abstractf+'data.txt')

# Create the tokenizer class
tokeniser = Tokenizer(add_bigram = False)

# Next create the dictionary by iterating over the abstracts, one per line in the txt file
dictionary = corpora.Dictionary(tokenizer.tokenize_and_stem(line) for line in open(abstractf+'.txt')) 
dictionary.save(abstractf+'_raw.dict')
{% endhighlight %}

The resulting dictionary  contains 959526 tokens. While  we could work
out a BoW model from there, it  is often a good idea to remove extreme
tokens. For  instance, a  token appearing  in only  1 abstract  is not
going to help us build a efficient recommandation system. Similarly, a
token  that appears  in  all  the documents  is  not  likely to  carry
relevant information neither for our purpose. I thereferore decided to
remove all  tokens that appear  in less than  5 abstracts and  in more
than 80% of them. Note that creating  the dictionarry can take up to 10
minute on my laptop which make serialization a good idea.

{% highlight python %}
dictionary =  corpora.Dictionary.load(abstractf+'_raw.dict')
dictionary.filter_extremes(no_below=5,no_above=0.80,keep_n=200000)
dictionary.id2token = {k:v for v,k in dictionary.token2id.iteritems()}
dictionary.save(abstractf+'.dict')
{% endhighlight %}

### BoW representation

Now we  have the  dictionary, it  is actually easy  to obtain  the BoW
representation of any document. We  just have to tokenize the document
using the  same function used  to build  the dictionary and  count the
occurence  of each  resulting  token.  Each  dictionary in  **gensim**
possess a method  **doc2bow** which does exactly that  and returns the
BoW representation as a sparse vector, i.e.  a vector where only words
that have a count different from zero are returned.

For instance, the BoW representation of my first abstract is


{% highlight python %}
my_contrib_bow = dictionary.doc2bow(tokenizer.tokenize_and_stem(abstracts[my_contrib[0]]))
df = [f+(dictionary.id2token[f[0]],) for f in my_contrib_bow]
df = pd.DataFrame(df,columns = ['id','occ','token']).sort_values(by='occ',ascending = False)
df.index= range(len(df))
df.head(5)
{% endhighlight %}


<iframe   width="500"   height="200"  frameborder="0"   scrolling="no"
src="https://plot.ly/~clement.thorey/37.embed"></iframe>

where the result are presented as  a pandas dataframe for clarity with
three columns,  the id  assigned for each  token by  the `Dictionnary`
class,  its count  and its  corresponding  token.  Note  that the  BoW
representation of my  abstract, which underlies the  importance of the
stem token  crater, lunar,  intrusion, floor  and classifi,  is farely
accurate.

By converting each abstract of the corpus using this `doc2bow` method,
we can obtain  the BoW representation of our full  corpus.  A careless
memory way  to do that  is to just iterate  the doc2bow method  of our
dictionary   over  the   abstract  list   we  have   defined  at   the
beginning. Nevertheless, this  would end up storing  the whole doc2bow
representation into memory as a huge matrice which can be problematic,
as least for my laptop.

Instead, **gensim** has been designed  such that it only requires that
a corpus must be able to return one document vector (for instance, the
doc2bow  representation of  the document  here)  at a  time.  We  then
define  the  BoW corpus  as  an  object  `MyCorpus` where  the  method
`__iter__` is consistently defined to  iter and transform each line of
a .txt file where the abstracts content is stored.

{% highlight python %}
class MyCorpus(Tokenizer):

    def __init__(self, name, add_bigram):
        super(MyCorpus, self).__init__(add_bigram)
        self.name = name

    def load_dict(self):
        if not os.path.isfile(self.name + '.dict'):
            print 'You should build the dictionary first !'
        else:
            setattr(self, 'dictionary',
                    corpora.Dictionary.load(self.name + '.dict'))

    def __iter__(self):
        for line in open(self.name + '_data.txt'):
            # assume there's one document per line, tokens separated by
            # whitespace
            yield self.dictionary.doc2bow(self.tokenize_and_stem(line))

bow_corpus = MyCorpus(abstractf)
corpora.MmCorpus.serialize(abstractf+'.mm',bow_corpus)
{% endhighlight %}

### Recommendation 

In the BoW representation of our corpus, each abstract is a point in a
high-dimensional embedding (a 14669 dimensions embedding exactly). The
*distance* or  the *similarity* between  one abstract and the  rest of
the corpus,  according to some  metrics, can  then be used  to compare
different contributions together and then, to provide a recommendation
list for a specific query.

The euclidean  distance is always a  first natural choice to  design a
distance  in an  arbitrary  space.  Given  two  vectors $\vec{a}$  and
$\vec{b}$, it is equal to

$$d(\vec{a},\vec{b})    =    \sqrt{(\vec{b}-    \vec{a})\cdot(\vec{b}-
\vec{a}) }$$

However, we'd like our distance to  be independant of the magnitude of
the  difference  between  two  vectors. For  instance,  we'd  like  to
identify  as similar  two  abstracts which  contain  exactly the  same
tokens even  if their  occurence differs significantly.  The euclidean
distance clearly does not have this property.

Accordingly, a  more reliable measure  for our purpose is  called "the
cosine  similarity". For  two  vectors, $\vec{a}$  and $\vec{b}$,  the
cosine similarity $d$ is defined as :

$$          d(\vec{a},\vec{b})=           \frac{\vec{a}          \cdot
\vec{b}}{|\vec{a}||\vec{b}|} = \cos(\vec{a},\vec{b})$$

In particular, this  similarity measure is the dot product  of the two
normalized vector and hence, depends only on the angle between the two
vectors (which is were its name comes  from ;). It ranges from -1 when
two vectors  point in the opposite  direction to 1 when  they point in
the same direction.

To compute the similarity of one query against our BoW representation,
the natural procedure is to  first transform our sparse representation
into its  dense equivalent, i.e. a  matrice where the number  of lines
correspond to the number of tokens in the dictionary and the number of
columns to  the number  of abstracts  in the  corpus. Then,  we column
normalize the  matrice such  that each document  correspond to  a unit
vector in the representation space. Finaly, we take the dot product of
the transposed  matrice with the  desired normalized query to  get its
cosine similarity agaist all the documents in the corpus.

**Gensim**  contains efficient  utility functions  to help  converting
  from/to numpy matrice and therefore, this translates to

{% highlight python %}
def get_score(doc_id):
    # First load the corpus and the dicitonary
    bow_corpus = corpora.MmCorpus(abstractf+'.mm')
    dictionary = corpora.Dictionary.load(abstractf+'.dict')
    # Transform our sparse representation into dense
    numpy_matrix = gensim.matutils.corpus2dense(bow_corpus, num_terms=len(dictionary))
    # Normalize each abstract in the corpus
    normalized_matrix = numpy_matrix/np.sqrt(np.sum(numpy_matrix*numpy_matrix,axis=0))
    # Take the dot product of the resulting matrice with query to get the relevant cosine similarity
    return np.dot(normalized_matrix.T,normalized_matrix[:,doc_id])
{% endhighlight %}

The   recommandation  against   my  abstracts   and  their   associate
cosine-similarity are

    Recom 1 - Cosine: 1.000 - Title: Floor-Fractured Craters through Machine Learning Methods
    Recom 2 - Cosine: 0.465 - Title: Preliminary Geological Map of the
    Ac-H-2 Coniraya Quadrangle of Ceres An Integrated Mapping Study Using Dawn Spacecraft Data
    Recom 3 - Cosine: 0.459 - Title: The collisional history of dwarf planet Ceres revealed by Dawn
    Recom 4 - Cosine: 0.453 - Title: Structural and Geological Interpretation of Posidonius Crater on the Moon
    Recom 5 - Cosine: 0.433 - Title: Initial Results from a Global Database of Mercurian Craters
    Recom 6 - Cosine: 0.431 - Title: Lunar Crater Interiors with High Circular Polarization Signatures

In  particular, the  cosine  similartiy of  the  query against  itself
returns 1, which in itself  is reassuring!  The cosine-similarity then
drops below 0.5. While the recommendations are farely accurate, I have
effectively  been most  of this  presentations,  we can  get a  slight
increase of the score using a trick called **tf-idf** normalization.

## TF-IDF representation

Indeed, one  of the  problem with  the BoW  representation is  that it
often puts  too much weights on  common words of the  corpus. While we
remove  most  common  words  of   the  english  language,  words  like
'present',   'show'   of  whatever   words   commonly   used  in   the
writing-abstract  vocabulary can  add  some noise  in  regards to  our
recommendation.  In particular here, we would like to put more weights
on tokens that make each abstract specific.

A  common way  to do  this  is to  use a  **Tf-Idf** normalization  to
re-weiht each count in the BoW  representation by the frequency of the
token  in   the  whole  corpus.  **Tf**   means  term-frequency  while
**Tfâ€“Idf** means term-frequency times inverse document-frequency. This
way, the weight  of common tokens in the corpus  will be significantly
lowered.

This  implentation  is  available  is **gensim**  and  can  be  easily
combined with the BoW representation  to get the representation of the
corpus in the tf-idf space.

{% highlight python %}
# First load the corpus and the dicitonary
bow_corpus = corpora.MmCorpus(abstractf+'.mm')
dictionary = corpora.Dictionary.load(abstractf+'.dict')
# Initialize the tf-idf model
tfidf = models.TfidfModel(bow_corpus)
# Compute the tfidf of the corpus itself
tfidf_corpus = tfidf[bow_corpus]
# Serialize both for reuse
tfidf.save(abstractf+'_tfidf.model')
corpora.MmCorpus.serialize(abstractf+'_tfidf.mm',tfidf_corpus)
{% endhighlight %}

In this  embedding, the cosine  similarity of my abstract  agaisnt the
remaining of the corpus gives

    Recom 1 - Cosine: 1.000 - Title:  Floor-Fractured Craters through Machine Learning Methods
    Recom 2 - Cosine: 0.488 - Title:  Structural and Geological Interpretation of Posidonius Crater on the Moon
    Recom 3 - Cosine: 0.475 - Title:  The collisional history of dwarf planet Ceres revealed by Dawn
    Recom 4 - Cosine: 0.462 - Title:  Katabatically Driven Downslope Windstorm-Type Flows on the Inner Sidewall of Arizona's Barringer Meteorite Crater
    Recom 5 - Cosine: 0.450 - Title:  Initial Results from a Global Database of Mercurian Craters
    Recom 6 - Cosine: 0.447 - Title:  Hydrological Evolution and Chemical Structure of the Hyper-acidic Spring-lake System on White Island, New Zealand

While the cosine-similarity  is indeed slightly better,  it still does
passes above 0.5.  One of the reason might be  the number of dimension
of the representation space. Indeed, both in the BoW and Tf-Idf model,
we  are trying  to  calculate  distance, similarity,  in  a very  high
dimensional space. One problem with a such huge number of dimension is
that similarity  measure begins  to becomes all  similar in  such high
embedding. A gain of performance could surely be obtained by trying to
reduce the size of the representation space.

## Latent Semantic Analysis (LSA) or (LSI)

And here comes  Latent Semantic Analysis (LSA) or  Indexing (LSI). LSI
is a common method in information retrieval to reduce the dimension of
the representation  space. The  idea behind  it is that  a lot  of the
dimensions  in  the  previous   representations  are  redundant.   For
instance,  the words  machine and  learning are  more likely  to occur
together. Therefore, shrinking these two  dimensions to only one which
is form  by a  linear combination  of the  token machine  and learning
would  reduce the  dimension without  any loss  of information.   More
generally, the Latent Semantic Analysis  aims to reduce the dimensions
while  keeping as  much  information possible  present  in the  higher
dimensal space by identifying deep semantic pattern in the corpus.

To identify this  semantic structure, Latent Semantic  Analysis used a
linear               algebra               method               called
[Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Latent_semantic_analysis). Formally,
the   SVD  is   able  to   identify  a   consistent  lower-dimensional
approximation  of  the   higher-dimensional  tfidf  space.  **Gensim**
implements  the   Latent  Semantic  Analysis  under   a  model  called
`LsiModel` which  can be  used on top  of our  previous representation
easily. It  requires a parameter, **num_topics**,  which corresponds to
the  desired   dimension  in  the   final  lsi  space.  I   settle  on
**num_topics=500** for good performance.

{% highlight python %}
    # First load the corpus and the dicitonary
    tfidf_corpus = corpora.MmCorpus(abstractf+'_tfidf.mm')
    dictionary = corpora.Dictionary.load(abstractf+'.dict')
    # Initialize the lsi model
    lsi = models.LsiModel(tfidf_corpus,id2word=dictionary, num_topics=500)
    # Compute the lsi of the corpus itself
    lsi_corpus = lsi[tfidf_corpus]
    # Serialize both for reuse
    lsi.save(abstractf+'_lsi.model')
    corpora.MmCorpus.serialize(abstractf+'_lsi.mm',lsi_corpus)
{% endhighlight %}

Indeed,  the  cosine   similarity  in  this  space   looks  much  more
promising !

    Recom 1 - Cosine: 1.000 - Title:  Floor-Fractured Craters through Machine Learning Methods
    Recom 2 - Cosine: 0.827 - Title:  Lunar Crater Interiors with High Circular Polarization Signatures
    Recom 3 - Cosine: 0.822 - Title:  Morphologic Analysis of Lunar Craters in the Simple-to-Complex Transition
    Recom 4 - Cosine: 0.815 - Title:  Continuous Bombardment  Effect of Small Primary and Secondary Impacts on the Lunar Regolith
    Recom 5 - Cosine: 0.813 - Title:  Structural and Geological Interpretation of Posidonius Crater on the Moon
    Recom 6 - Cosine: 0.780 -  Title: Comparing Radar and Optical Data Sets of Lunar Impact Crater Ejecta







